{
 "cells": [
  {
   "source": [
    "# Importation de `torch`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Tenseurs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Initialisation de tenseurs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### À partir de zéros et de uns ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 0.],\n        [0., 0.]])\ntorch.float32\ntensor([[1, 1],\n        [1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "t = torch.zeros(2, 2)\n",
    "x = torch.ones(2, 2, dtype=torch.int)\n",
    "print(t)\n",
    "print(t.dtype)\n",
    "print(x)\n"
   ]
  },
  {
   "source": [
    "Par défaut `torch` spécifie le type de donnée par `float32`. On peut changer le type de donnée avec `dtype=...`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Aléatoirement ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.1304, 0.5134],\n        [0.7426, 0.7159]])\ntensor([[0.5705, 0.1653],\n        [0.0443, 0.9628]])\ntensor([[0.1304, 0.5134],\n        [0.7426, 0.7159]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2021)\n",
    "t = torch.rand(2, 2)\n",
    "x = torch.rand(2, 2)\n",
    "\n",
    "torch.manual_seed(2021)\n",
    "y = torch.rand(2, 2)\n",
    "\n",
    "print(t)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "On peut utiliser une `seed` pour retrouver le même tenseur générer aléatoirement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### À partir de données ###"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "source": [
    "### À partir de `numpy` ###\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "source": [
    "### À partir d'un autre tenseur ### "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ones Tensor: \n tensor([[1, 1],\n        [1, 1]]) \n\nRandom Tensor: \n tensor([[0.5705, 0.1653],\n        [0.0443, 0.9628]]) \n\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # même propriétés que x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # redéfinit datatype de x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "source": [
    "## Les propriétés des tenseurs ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Forme : torch.Size([3, 4])\nDatatype : torch.float32\nStockage : cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Forme : {tensor.shape}\")\n",
    "print(f\"Datatype : {tensor.dtype}\")\n",
    "print(f\"Stockage : {tensor.device}\")"
   ]
  },
  {
   "source": [
    "## Opérations sur des tenseurs ##"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Opération basique"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A random matrix, r:\ntensor([[-0.1950, -0.6964],\n        [-0.0058, -0.4975]])\n\nAbsolute value of r:\ntensor([[0.1950, 0.6964],\n        [0.0058, 0.4975]])\n\nInverse sine of r:\ntensor([[-0.1963, -0.7703],\n        [-0.0058, -0.5208]])\n\nDeterminant of r:\ntensor(0.0930)\n\nSingular value decomposition of r:\ntorch.return_types.svd(\nU=tensor([[ 0.8271,  0.5620],\n        [ 0.5620, -0.8271]]),\nS=tensor([0.8713, 0.1068]),\nV=tensor([[-0.1889, -0.9820],\n        [-0.9820,  0.1889]]))\n\nAverage and standard deviation of r:\n(tensor(0.3078), tensor(-0.3487))\n\nMaximum value of r:\ntensor(-0.0058)\n"
     ]
    }
   ],
   "source": [
    "r = torch.rand(2, 2) - 0.5 * 2 # values between -1 and 1\n",
    "print('A random matrix, r:')\n",
    "print(r)\n",
    "\n",
    "# Common mathematical operations are supported:\n",
    "print('\\nAbsolute value of r:')\n",
    "print(torch.abs(r))\n",
    "\n",
    "# ...as are trigonometric functions:\n",
    "print('\\nInverse sine of r:')\n",
    "print(torch.asin(r))\n",
    "\n",
    "# ...and linear algebra operations like determinant and singular value decomposition\n",
    "print('\\nDeterminant of r:')\n",
    "print(torch.det(r))\n",
    "print('\\nSingular value decomposition of r:')\n",
    "print(torch.svd(r))\n",
    "\n",
    "# ...and statistical and aggregate operations:\n",
    "print('\\nAverage and standard deviation of r:')\n",
    "print(torch.std_mean(r))\n",
    "print('\\nMaximum value of r:')\n",
    "print(torch.max(r))"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Standard numpy-like indexing and slicing:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "source": [
    "### Concaténation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "source": [
    "### Multiplication"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor.mul(tensor) \n tensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]]) \n\ntensor * tensor \n tensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# This computes the element-wise product\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "source": [
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# Alternative syntax:\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor.matmul(tensor.T) \n tensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]) \n\ntensor @ tensor.T \n tensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\n"
     ]
    }
   ]
  },
  {
   "source": [
    "**In-place operations**\n",
    "Operations that have a ``_`` suffix are in-place. For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]]) \n\ntensor([[6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "source": [
    "Si on modifie un tenseur stocké sur le cpu, la même matrice `numpy` sera changée car elle est également stokée sur le cpu et vice-versa."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\nn: [1. 1. 1. 1. 1.]\nt: tensor([2., 2., 2., 2., 2.])\nn: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "source": [
    "# Autograd"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exemple de RNN (recurent neural network)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 10, requires_grad=True) #tenseur d'entrée\n",
    "prev_h = torch.randn(1 ,20) #couche cachée\n",
    "w_h = torch.randn(20 ,20) #poids de la couche cachée\n",
    "w_x = torch.randn(20 ,10) #poids du tenseur d'entrée\n",
    "\n",
    "i2h = torch.mm(w_x, x.t()) #multiplication de x et w_x (mm : matrix multiplication)\n",
    "h2h = torch.mm(w_h, prev_h.t())\n",
    "\n",
    "next_h = i2h + h2h #somme des multiplications\n",
    "next_h = next_h.tanh() #fonction d'activitaion \n",
    "\n",
    "loss = next_h.sum() #fonction à optimiser\n",
    "loss.backward() #calcul les gradients pour la fonction loss"
   ]
  },
  {
   "source": [
    "## Exemple d'étape d'entrainement"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True) #model préentrainé\n",
    "data = torch.rand(1, 3, 64, 64) # tenseur aléatoire représentant une image 3D de résolution 64x64\n",
    "labels = torch.rand(1, 1000)\n",
    "\n",
    "prediction = model(data) #passe avant, on entre les données dans le modèle afin d'obtenir ses prédictions\n",
    "\n",
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # passe arrière\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "optim.step() #gradient descent"
   ]
  },
  {
   "source": [
    "## Exemple de calcul du gradient avec autograd"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "source": [
    "`requires_grad=True` implique que toutes les opérations appliquées sur le tenseur seront suivies par autograd"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2 \n",
    "\n",
    "external_grad = torch.tensor([1., 1.]) #le gradient de Q selon lui même est égale à 1\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "source": [
    "Comme `Q` est un vecteur, on doit spécifier le gradient lorsqu'on utilise `.backward()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([True, True])\ntensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "source": [
    "Comme l'égalité est respectée entre les dérivées partielles de `Q` et `a.grad()` ou `b.grad()` on a la preuve que autograd a bien fonctionné."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Exemple de NN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "source": [
    "### Définition du réseaux"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Net(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "### Passe avant (forward pass) et arrière (backward pass)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10\ntorch.Size([6, 1, 5, 5])\ntensor([[-0.0235,  0.0806,  0.0334,  0.0452, -0.0450, -0.0410,  0.0937,  0.1087,\n         -0.1257,  0.0126]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)\n",
    "\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "source": [
    "### Fonction objectif (loss function) - Erreur moyenne carrée"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.0728, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss() # mean square error\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "source": [
    "## Backprop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "conv1.bias.grad before backward\ntensor([0., 0., 0., 0., 0., 0.])\nconv1.bias.grad after backward\ntensor([-0.0255,  0.0324,  0.0317,  0.0303, -0.0104, -0.0232])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "source": [
    "## Update the weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "498cdea74d3331ee4660c3102ac8d2ad0b4bad9948a4fe9d378951ec0116e2dc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}